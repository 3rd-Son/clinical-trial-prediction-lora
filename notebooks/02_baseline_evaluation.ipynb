{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Evaluation: Zero-Shot Llama-3-8B\n",
    "\n",
    "This notebook evaluates the **baseline performance** of Llama-3-8B on clinical trial outcome predictions without any fine-tuning.\n",
    "\n",
    "## Overview\n",
    "\n",
    "We'll test how well a vanilla language model can predict clinical trial outcomes using only its pre-trained knowledge. This establishes our baseline to compare against fine-tuned performance.\n",
    "\n",
    "## What We're Testing\n",
    "\n",
    "- **Model:** Llama-3-8B (zero-shot, no training)\n",
    "- **Task:** Binary prediction (will trial succeed: YES/NO)\n",
    "- **Test Set:** 206 held-out questions\n",
    "- **Evaluation:** Accuracy, confusion matrix, error analysis\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- ‚ö†Ô∏è **GPU Required** - Use Google Colab with T4 runtime (free)\n",
    "- Hugging Face account with Llama-3 access ([request here](https://huggingface.co/meta-llama/Meta-Llama-3-8B))\n",
    "- Test dataset from notebook 01\n",
    "\n",
    "## Expected Results\n",
    "\n",
    "- **Baseline accuracy:** ~55-58% (slightly better than random)\n",
    "- **Common issue:** Optimistic bias (predicts success too often)\n",
    "- **Runtime:** ~18-23 minutes\n",
    "\n",
    "## Output Files\n",
    "\n",
    "- `baseline_results.csv` - All baseline predictions with correctness labels\n",
    "\n",
    "---\n",
    "\n",
    "**Note:** Don't run this on your laptop unless you have a dedicated GPU. Use Google Colab!\n",
    "\n",
    "Let's establish our baseline! üìä"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162,
     "referenced_widgets": [
      "18a5208f38134e56853aa10e792b6976",
      "ecdfa9000cd94b07b270e5f8d3411c04",
      "42c814f5b098439abb69aad4db5822ec",
      "cdaba78c03674c7588cf4a43d4a96bd6",
      "b3bfbaa3b79045789540ab1498b2e986",
      "b9cbc43e8deb48288b4eaa94a2e19c6f",
      "4c2a14d68dda4f0ebd4eeeb3cb8622c7",
      "194efab020364bb3b56dadbad3e3b8a7",
      "1eeb0276c31b4c65afe1192645f4f684",
      "1385aba163ab433aabcf215349d0a6f3",
      "b5db58a3fa0f4f91999823105a18b588",
      "1f5062d0ff804eaaa08a67b9e9b211d9",
      "4214b2b2093d4a13907ca09dce9b803c",
      "829ba33bb32448bc961b5fed185d076e",
      "397df6bf75f14401a83d76dc69136fb6",
      "430017e2127c47719fa24f6b969e86ec",
      "7d073040346f4970ab0dc81f815fd5da",
      "93d851e72a9a4eac923e84b01a2579dd",
      "f31b3ed060984faca0592a0f262c873a",
      "b2723915e58745b2a539a2c130949fd4"
     ]
    },
    "id": "isXw6ROnocrl",
    "outputId": "92911803-9705-421f-ed6f-b9c06188c9ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m60.7/60.7 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18a5208f38134e56853aa10e792b6976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install -q transformers accelerate bitsandbytes huggingface_hub\n",
    "\n",
    "# Login to Hugging Face\n",
    "from huggingface_hub import login\n",
    "login()  # This will prompt you to enter your token\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load test data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376,
     "referenced_widgets": [
      "20be758859bf47978b185fa30e1b936e",
      "4a402ab56d0e41dc9072fc09163d2659",
      "69bbfc116bd149ad88f89848abaf831b",
      "9dc380610f254f07a0dfcbfdffac3d29",
      "b2ebe281f26f42b89a1fba2e2f4947bf",
      "f5d60d4d43b94b698b7bc752714f9609",
      "33c0f98b283d4202a3be5eb0d140be3d",
      "cc1ba90b996d4273a89f98d885e79208",
      "e409db0ad6794067b8472cd367b5f3fb",
      "841b003be85444d7bc2021e7223113e3",
      "215d0f34f8d144ed9d93500bd6a57391",
      "5c1173aaf3fc4998a6bb5bb7c93b3131",
      "b96c5bfce5c749c0a85acc25f634e0f9",
      "129e08451a964d99b47b5610d239fd2a",
      "aae2fa744c1649eea07ecea4662fcb3d",
      "e69aba87f8dd4bc6b4f04ad70b2f8bdc",
      "2ba2473624d54399bd68c9e625330e9d",
      "309a3c47d5c34726b15f098f7d7b9dbf",
      "59aabcef1a584f6980073f0b476e56c1",
      "012743eecc124dbd867257267f258dd4",
      "6c37e85d89d04a83ac025b911d17558f",
      "8773ed315d1242e1822bb50f46429fa4",
      "9448dc3bde28429499135ad822bbc65f",
      "45b7bd1aecd54d87a7604765766a0221",
      "0ebc3d435a464ffebced0f12301b3334",
      "a7a561f0c57a429898c72d33ee75467f",
      "2b59ba7b83a94f4a81b23321d72ac3dc",
      "a4f6985ae3b942c9b3dc6b4108c5e3d9",
      "6719db6d8fef4c688eb8cdb3ef2c9d4f",
      "91b12a5f754048dd8a566a8b9020846a",
      "7fc49cf8a26047f6b0c66e88703cec7b",
      "7ccbe1c0a0cc4ca3b91d52fb3d1e6f6d",
      "53131594265544b1bb5ce1c06ef33129",
      "b1cbd33522944f068edbc4a29f79874b",
      "3c7c0a67b3224bb09b2e97f495bcc8a0",
      "9e0b863479654b06b2c0bac86945232c",
      "41dc8e4d9df445e6bd40ad71b980dff2",
      "9dc708fa08d34f7882e69593f4e09e0a",
      "b1db930d15534f02815d912cf56e25e8",
      "394c8313954545d9834be6396c4286e2",
      "f893e9e54c0f436f8b48a85b50e8a6c9",
      "e2d9f0dd2902494d93c69835bb53a3af",
      "68df724a8f0741d78a673917ffc5d397",
      "f0458c290ca24c3f94d5241fce681b6f",
      "3170435c5a734103b5d0fd2006a69ac0",
      "e8778ed058a54c14951bfee0ace047ab",
      "2e0cc149aaec49f7b122523def34342e",
      "c4ac8a43c13a4a5c9964e6d333be8d79",
      "e70e854a286344419b908633dad26c44",
      "fb7afa5a42a7461da4f72b52972a7106",
      "d5183c89edc34219bb5437922ecc321e",
      "8cd2c6efb5334cadb5cc5ff682e49fbb",
      "9c24dd83df00434b8282a032de0fba0c",
      "8bd557c55bbe4844896cba2ec00df162",
      "15982385214a4979af9d2befb741ba36",
      "e74048556fa24a9f9d53ffbf7a30889a",
      "070c9abf14044c9da5cab500ed4b77b6",
      "3004a9bc00934aa6a3c8df43151c07b6",
      "5b7b04f8f55d4666a9ccf735d296b3d8",
      "c77086add92e4a5ba0e22f75328c35d4",
      "3bb2017277c445479963e992246ab32a",
      "2b36f8e0d5ea414585398438dd8d4996",
      "7315b502a7db4fe1aa5e4be876a4df2e",
      "19c64c73ef8f4822a2fdc00a7e02fa68",
      "e2be2a6649044a248397c7c9fd66cf46",
      "efd455335fa54793afce987981251d04",
      "d0b345e4859f46f784fc3c7a55e8b084",
      "225dd49dad6247909076da3c165c4c34",
      "46f582825f7344f584d7a7fe9ee092d2",
      "f986de224686424ab4a3db39160c866c",
      "1d4a714f335648f491b8fa1e39ec7f37",
      "4fc5f047da8746d2a665e34c4022687a",
      "e522079296d640aeaaedb4cbabee7246",
      "cf8b7a8f9c384293a6e8dc7c3e14698a",
      "73341b9449284db19840a4116d62921f",
      "5f59feaf5e82402ea4a119bb58123aad",
      "63ffb190078a433b87148e69ce94f546",
      "a180e75b2bd14f7c9a7ae8648d1a906d",
      "23c93cb270284713893abc45ab28de70",
      "547f3a0d5eb04b13ad94b7e771fc83cc",
      "5c4c3fb7dc07424ca5dd03d119c6b550",
      "7acb7fe2b55540009a478855b4c244d8",
      "60992ebabd0b4688a9753c5119012392",
      "435295dedbe1469c86b7abc2a389bd8d",
      "31ec2a6dc97941168d296dc7ec73a691",
      "927cc1193dee4b96932589f36aa3ca9d",
      "46abdd90858d4d018ab922cff675e053",
      "49f55e1088d346bfb74b4ae19359a496",
      "a6391790c7b346de94723d3555990b5d",
      "e766e40d23c844d68fcf2dfe64987d1b",
      "06c00f17538948b8a83aa41ff863b107",
      "f1e79fc8c1b6465f9d194310a52766b8",
      "72d96727f2da42bd98765da0c1efabee",
      "c110bf08bde8440b9207807d4c16dc39",
      "6a53d2271bdd43b5b3f67769eaa58afc",
      "d72b1235ac64460abf349e817d495ae5",
      "f914a678b43841a8a6c8d1f1eaa8fe7a",
      "65a0026b09434f7a9854e0b196d447b1",
      "1e8a73894a1c4e7ba177c547e25aa5c8"
     ]
    },
    "id": "dvjAfdyhon7O",
    "outputId": "295ce998-1265-43b9-cf17-47d8b6030bc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 206 test samples\n",
      "\n",
      "Loading meta-llama/Meta-Llama-3-8B...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20be758859bf47978b185fa30e1b936e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c1173aaf3fc4998a6bb5bb7c93b3131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9448dc3bde28429499135ad822bbc65f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1cbd33522944f068edbc4a29f79874b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3170435c5a734103b5d0fd2006a69ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e74048556fa24a9f9d53ffbf7a30889a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0b345e4859f46f784fc3c7a55e8b084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a180e75b2bd14f7c9a7ae8648d1a906d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6391790c7b346de94723d3555990b5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/177 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded!\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"clinical_test.csv\")\n",
    "print(f\"Loaded {len(test_df)} test samples\")\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "print(f\"\\nLoading {model_name}...\")\n",
    "\n",
    "# Configure 8-bit quantization to save memory\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Set pad token if not set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"‚úÖ Model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to create prompt & extract prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z4k-26pso2tH",
    "outputId": "65984b28-79aa-40d4-93ff-6fd503f791ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Running baseline evaluation (this takes ~10-15 minutes)...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_baseline_prompt(question):\n",
    "    return f\"\"\"You are evaluating clinical trial outcomes. Based on the question below, predict whether the outcome will be YES (1) or NO (0).\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Respond with only a single digit: 0 or 1.\n",
    "Answer:\"\"\"\n",
    "\n",
    "\n",
    "def extract_prediction(text):\n",
    "    # Look for 0 or 1 in the response\n",
    "    text = text.strip()\n",
    "    match = re.search(r'\\b[01]\\b', text)\n",
    "    if match:\n",
    "        return int(match.group())\n",
    "    return None\n",
    "\n",
    "# Run baseline evaluation\n",
    "predictions = []\n",
    "correct = 0\n",
    "total = 0\n",
    "unparseable = 0\n",
    "\n",
    "print(\"\\nüîç Running baseline evaluation (this takes ~2-3 minutes)...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V-D_78JWo9Zv",
    "outputId": "4bb0c71f-8889-410d-b7cc-282f660b3dd4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/206 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 206/206 [03:45<00:00,  1.09s/it]\n"
     ]
    }
   ],
   "source": [
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "    prompt = create_baseline_prompt(row['Question'])\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=10,\n",
    "            temperature=0.1,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    prediction = extract_prediction(response)\n",
    "\n",
    "    if prediction is not None:\n",
    "        predictions.append({\n",
    "            'Question': row['Question'],\n",
    "            'true_answer': row['Answer'],\n",
    "            'predicted_answer': prediction,\n",
    "            'correct': prediction == row['Answer'],\n",
    "            'raw_response': response.strip()\n",
    "        })\n",
    "\n",
    "        if prediction == row['Answer']:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    else:\n",
    "        unparseable += 1\n",
    "        predictions.append({\n",
    "            'Question': row['Question'],\n",
    "            'true_answer': row['Answer'],\n",
    "            'predicted_answer': None,\n",
    "            'correct': False,\n",
    "            'raw_response': response.strip()\n",
    "        })\n",
    "\n",
    "# Calculate accuracy\n",
    "baseline_accuracy = correct / total if total > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print results and save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zeQTcQX3oJpI",
    "outputId": "baabe04e-6d3a-42bf-bcef-473f1cc4d80f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "BASELINE EVALUATION RESULTS\n",
      "================================================================================\n",
      "Model: meta-llama/Meta-Llama-3-8B\n",
      "Total test samples: 206\n",
      "Successfully parsed: 206\n",
      "Unparseable responses: 0\n",
      "Correct predictions: 116\n",
      "Baseline Accuracy: 56.3%\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Results saved to baseline_results.csv\n",
      "\n",
      "üìã Sample predictions:\n",
      "                                            Question  true_answer  \\\n",
      "0  Will Pharnext announce positive topline result...            0   \n",
      "1  Will Sarepta Therapeutics release results from...            1   \n",
      "2  Will EIP Pharma (CervoMed) complete its Phase ...            1   \n",
      "3  Will argenx SE receive FDA approval for VYVGAR...            1   \n",
      "4  Will Capricor Therapeutics report top-line dat...            0   \n",
      "5  Will the FDA approve AbbVie's Rinvoq (upadacit...            1   \n",
      "6  Will Reata Pharmaceuticals' Skyclarys (omavelo...            1   \n",
      "7  Will Biogen begin patient screening for its Ph...            0   \n",
      "8  Will AbbVie's Rinvoq (upadacitinib) receive FD...            0   \n",
      "9  Will UCB's Bimzelx (bimekizumab-bkzx) be comme...            1   \n",
      "\n",
      "   predicted_answer  correct  \n",
      "0                 1    False  \n",
      "1                 1     True  \n",
      "2                 1     True  \n",
      "3                 1     True  \n",
      "4                 1    False  \n",
      "5                 1     True  \n",
      "6                 1     True  \n",
      "7                 1    False  \n",
      "8                 1    False  \n",
      "9                 1     True  \n",
      "\n",
      "================================================================================\n",
      "DETAILED STATISTICS\n",
      "================================================================================\n",
      "Answer distribution in predictions:\n",
      "predicted_answer\n",
      "1    201\n",
      "0      5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "True answer distribution:\n",
      "Answer\n",
      "1    113\n",
      "0     93\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BASELINE EVALUATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Total test samples: {len(test_df)}\")\n",
    "print(f\"Successfully parsed: {total}\")\n",
    "print(f\"Unparseable responses: {unparseable}\")\n",
    "print(f\"Correct predictions: {correct}\")\n",
    "print(f\"Baseline Accuracy: {baseline_accuracy:.1%}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save results\n",
    "results_df = pd.DataFrame(predictions)\n",
    "results_df.to_csv(\"baseline_results.csv\", index=False)\n",
    "print(\"\\n‚úÖ Results saved to baseline_results.csv\")\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\nüìã Sample predictions:\")\n",
    "display_df = results_df[['Question', 'true_answer', 'predicted_answer', 'correct']].head(10)\n",
    "print(display_df)\n",
    "\n",
    "# Additional statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "if total > 0:\n",
    "    print(f\"Answer distribution in predictions:\")\n",
    "    print(results_df[results_df['predicted_answer'].notna()]['predicted_answer'].value_counts())\n",
    "    print(f\"\\nTrue answer distribution:\")\n",
    "    print(test_df['Answer'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}